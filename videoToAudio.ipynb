{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import moviepy.editor as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in Audio/audio.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "video_clip = mp.VideoFileClip(\"Family.mp4\")\n",
    "audio_clip = video_clip.audio\n",
    "\n",
    "audio_clip.write_audiofile(\"Audio/audio.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.25.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'Wav2Vec2CTCTokenizer'. \n",
      "The class this function is called from is 'Wav2Vec2Tokenizer'.\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2Tokenizer, Wav2Vec2ForCTC\n",
    "import librosa as lb\n",
    "import torch\n",
    "\n",
    "tokenizer = Wav2Vec2Tokenizer.from_pretrained('facebook/wav2vec2-base-960h')\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained('facebook/wav2vec2-base-960h')\n",
    "\n",
    "audioWave, rate = lb.load('./Audio/audio.wav', sr = 16000)\n",
    "input_values = tokenizer(audioWave, return_tensors='pt').input_values\n",
    "\n",
    "logits = model(input_values).logits\n",
    "\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = tokenizer.batch_decode(predicted_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"videoText.txt\", \"w\")\n",
    "file.write(*transcription)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "\n",
    "# summarizer = pipeline(\"summarization\")\n",
    "# summary = summarizer(transcription, max_length=300, min_length = 50, do_sample=\"False\")[0]['summary_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190\n"
     ]
    }
   ],
   "source": [
    "file = open(\"videoText.txt\", \"r\")\n",
    "text = file.read()\n",
    "pattern = \"COUNTRYSIDE\"\n",
    "\n",
    "# Define the Base value. Here I am using d = 127 to represent all characters\n",
    "base = 127\n",
    "\n",
    "# Define the size of text and pattern\n",
    "textSize = len(text)\n",
    "patternSize = len(pattern)\n",
    "m = patternSize\n",
    "\n",
    "prime = 117\n",
    "# Calculate the Hash value using the hash function for pattern\n",
    "\n",
    "def calculateHash(patt):\n",
    "    hashValue = -1\n",
    "    m = patternSize\n",
    "    for char in patt:\n",
    "        hashValue += ord(char) * (base ** (m-1))\n",
    "        m -= 1\n",
    "\n",
    "    hashValue = hashValue % prime\n",
    "    return hashValue\n",
    "\n",
    "## Finding the pattern match\n",
    "\n",
    "patternHash = calculateHash(pattern)\n",
    "\n",
    "start = 0\n",
    "while start < textSize-patternSize+1:\n",
    "    curr_hash = calculateHash(text[start:start+patternSize])\n",
    "    if curr_hash == patternHash and text[start:start+patternSize] == pattern: print(start)\n",
    "\n",
    "    start += 1\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
